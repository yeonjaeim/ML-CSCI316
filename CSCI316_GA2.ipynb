{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 316 - Group Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Main Features of Pyspark\n",
    "### • Transformers– functions that convert raw data in some way. \n",
    "### • Estimators– if provided with data, result in transformers– algorithms that are used to train models\n",
    "### • Evaluators– evaluate how a given model performs according to criteria\n",
    "### • Pipeline– MLlib’s highest-level data type– Transformers, estimators and evaluators are all stages in a pipeline\n",
    "# * We will actively utilize the four features mentioned above to gain a deep understanding of PySpark.\n",
    "# -------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "## when there is 'str object is not callable' error\n",
    "## please add 'from pyspark.sql.functions import col'\n",
    "## sometimes 'col' can't be recognized even though it was imported before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Transformer\n",
    "\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
    "import pandas as pd\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"a2\").config(\"spark.sql.execution.pythonUDF.arrow.enabled\", \"false\").getOrCreate()\n",
    "\n",
    "# read file\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"train.csv\")\n",
    "\n",
    "# exclude ID and Name --> NOT RELATED TO PREDICT CREDIT SCORE IN COMMON SENSE\n",
    "selected_columns = [col_name for col_name in df.columns if col_name not in [\"ID\", \"Name\"]]\n",
    "df = df.select(selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reason for the worse performance after repartitioning  during the data preprocessing stage, is as follows:\n",
    "\n",
    "### Data Movement Cost: Repartitioning involves physically moving data, incurring additional time and resource consumption. This data movement cost can lead to increased processing time.\n",
    "\n",
    "### Hence, We will not use repartitioning during data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition\n",
    "#num_partitions = 125\n",
    "#df = df.repartition(num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SSN: string (nullable = true)\n",
      " |-- Occupation: string (nullable = true)\n",
      " |-- Annual_Income: string (nullable = true)\n",
      " |-- Monthly_Inhand_Salary: string (nullable = true)\n",
      " |-- Num_Bank_Accounts: string (nullable = true)\n",
      " |-- Num_Credit_Card: string (nullable = true)\n",
      " |-- Interest_Rate: string (nullable = true)\n",
      " |-- Num_of_Loan: string (nullable = true)\n",
      " |-- Type_of_Loan: string (nullable = true)\n",
      " |-- Delay_from_due_date: string (nullable = true)\n",
      " |-- Num_of_Delayed_Payment: string (nullable = true)\n",
      " |-- Changed_Credit_Limit: string (nullable = true)\n",
      " |-- Num_Credit_Inquiries: string (nullable = true)\n",
      " |-- Credit_Mix: string (nullable = true)\n",
      " |-- Outstanding_Debt: string (nullable = true)\n",
      " |-- Credit_Utilization_Ratio: string (nullable = true)\n",
      " |-- Credit_History_Age: string (nullable = true)\n",
      " |-- Payment_of_Min_Amount: string (nullable = true)\n",
      " |-- Total_EMI_per_month: string (nullable = true)\n",
      " |-- Amount_invested_monthly: string (nullable = true)\n",
      " |-- Payment_Behaviour: string (nullable = true)\n",
      " |-- Monthly_Balance: string (nullable = true)\n",
      " |-- Credit_Score: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datatypes are automatically indicated as string\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Labels\n",
    "### The table, as seen above, contains only four numerical columns. Columns are classified into three categories:\n",
    "\n",
    "#### label: the label of the data ('Credit_Score') used for classification;\n",
    "#### categorical_cols: the names of the 7 categorical columns;\n",
    "#### numeric_cols: the remanining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Credit_Score'\n",
    "categorical_cols = ['Customer_ID', 'Month', 'SSN', 'Occupation',\n",
    "                    'Credit_Mix', 'Type_of_Loan', 'Payment_of_Min_Amount',\n",
    "                    'Payment_Behaviour']\n",
    "numeric_cols = list(set(df.columns) - set(categorical_cols) -set([label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Later, we will complete basic data preprocessing tasks and change the data type of all columns to double for smooth model implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer & Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will used at step 6\n",
    "# This transformer generates predicted values for missing values in specific columns.\n",
    "class MissingValuePredictor(Transformer):\n",
    "    \"\"\"\n",
    "    Transformer to predict missing values in the target variable using a specified model,\n",
    "    and fill the missing values in the DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, target_col, join_cols, model):\n",
    "        \"\"\"\n",
    "        Initializes the MissingValuePredictor with target, input, join columns, and model.\n",
    "        \"\"\"\n",
    "        super(MissingValuePredictor, self).__init__()\n",
    "        self.target_col = target_col\n",
    "        self.join_cols = join_cols\n",
    "        self.model = model\n",
    "        \n",
    "    def _transform(self, df):\n",
    "        \"\"\"\n",
    "        Fits the specified model on non-missing data,\n",
    "        predicts missing values, fills them, and returns the DataFrame.\n",
    "        \"\"\"\n",
    "        # Step 3: Predict missing values\n",
    "        df_missing = df.filter(col(self.target_col).isNull())\n",
    "        df_missing = df_missing.select(self.join_cols)\n",
    "        predictions_missing = self.model.transform(df_missing)\n",
    "\n",
    "        # Step 4: Fill missing values in the original DataFrame\n",
    "        predicted_df = predictions_missing.select(*(self.join_cols + [\"prediction\"]))\n",
    "        df = df.join(predicted_df, on=self.join_cols, how=\"left_outer\")\n",
    "        df = df.withColumn(self.target_col,\n",
    "                           when(col(self.target_col).isNull(), col(\"prediction\"))\n",
    "                           .otherwise(col(self.target_col)))\n",
    "        df = df.drop(\"prediction\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "class SparkDFPipeline:\n",
    "    \"\"\"\n",
    "    This pipeline is classified into three cases based on the parameter 'case'.\n",
    "\n",
    "    case = 1:\n",
    "    Scaling is applied to all columns.\n",
    "\n",
    "    case = 2:\n",
    "    Scaling is applied only to continuous variables.\n",
    "\n",
    "    case = else:\n",
    "    No scaling is applied.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, case=0):\n",
    "        self.case = case\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, indexed, feature_list, to_remove):\n",
    "        \"\"\"\n",
    "        indexed: dataframe\n",
    "        feature_list: whole features in the dataframe. will be filtered to be the proper independent vars.\n",
    "        to_remove: the list of columns that will be removed from feature_list\n",
    "        \"\"\"\n",
    "        # apply scaling to all the vairables\n",
    "        if self.case == 0:\n",
    "            # Repartition\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = indexed.repartition(num_partitions)\n",
    "\n",
    "            for col in feature_list:\n",
    "                if col in to_remove:\n",
    "                    feature_list.remove(col)\n",
    "\n",
    "            # Vectorize the df\n",
    "            assembler = VectorAssembler(inputCols=feature_list, outputCol='features')\n",
    "            features_vectorized = assembler.transform(repartitioned_df)\n",
    "\n",
    "            # Standardize the df\n",
    "            scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "            scaled_data = scaler.fit(features_vectorized)\n",
    "            preprocessed_data = scaled_data.transform(features_vectorized)\n",
    "            \n",
    "\n",
    "        # apply scaling only to continuous variables\n",
    "        elif self.case == 1:\n",
    "            # Resample the df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = indexed.repartition(num_partitions)\n",
    "            \n",
    "            feature_list = numeric_cols\n",
    "            for col in feature_list:\n",
    "                if col in to_remove:\n",
    "                    feature_list.remove(col)\n",
    "\n",
    "            # Vectorize the df\n",
    "            assembler = VectorAssembler(inputCols=feature_list, outputCol='features')\n",
    "            features_vectorized = assembler.transform(repartitioned_df)\n",
    "\n",
    "            # Standardize the df\n",
    "            scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "            scaled_data = scaler.fit(features_vectorized)\n",
    "            preprocessed_data = scaled_data.transform(features_vectorized)\n",
    "\n",
    "            preprocessed_data = preprocessed_data.drop('features')\n",
    "            \n",
    "            # put back the categorical columns\n",
    "            input_cols = []\n",
    "           \n",
    "            for col in indexed.columns:\n",
    "                if '_Index' in col:\n",
    "                    input_cols.append(col)\n",
    "            \n",
    "            assembler1 = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "            preprocessed_data = assembler1.transform(preprocessed_data)\n",
    "            \n",
    "            \n",
    "        # No Standardization   \n",
    "        else:\n",
    "            # Normal vectorize df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = indexed.repartition(num_partitions)\n",
    "            for col in feature_list:\n",
    "                if col in to_remove:\n",
    "                    feature_list.remove(col)\n",
    "            assembler = VectorAssembler(inputCols=feature_list, outputCol='features')\n",
    "            preprocessed_data = assembler.transform(repartitioned_df)\n",
    "\n",
    "\n",
    "        return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate various kind of score for the test set and show the evaluation metrics\n",
    "def evaluate_model(model, test_data):\n",
    "    # Make predictions on the validation data\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = acc_evaluator.evaluate(predictions)\n",
    "\n",
    "    f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "    f1_score = f1_evaluator.evaluate(predictions)\n",
    "\n",
    "    precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "    precision_score = precision_evaluator.evaluate(predictions)\n",
    "        \n",
    "    recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "    recall_score = recall_evaluator.evaluate(predictions)\n",
    "\n",
    "    # Create a dictionary of model evaluation metrics\n",
    "    eval_metrics = {\n",
    "        'Accuracy': accuracy\n",
    "        , 'F1 Score': f1_score\n",
    "        , 'Precision': precision_score\n",
    "        , 'Recall': recall_score\n",
    "    }\n",
    "    \n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING\n",
    "### Single Process:\n",
    "### If the code is used only in a single task or specific process, there is no need to create a Transformer.\n",
    "### Duplicate Processes:\n",
    "### Using a Transformer increases code reusability and helps in building pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip underscore: convert \"_\" to empty string\n",
    "for column in df.columns:\n",
    "    df = df.withColumn(column, regexp_replace(col(column), \"_\", \"\"))\n",
    "\n",
    "# Remove two meaningless string values from the whole data set\n",
    "strings_to_replace = [\"#F%$D@*&8\", \"!@9#%8\"]\n",
    "for column in df.columns:\n",
    "    for string_to_replace in strings_to_replace:\n",
    "        df = df.withColumn(column, regexp_replace(col(column), string_to_replace, \"\"))\n",
    "\n",
    "# Since after done with previous work, we got some empty string values\n",
    "# Hence, we need to convert the empty string to null        \n",
    "for column in df.columns:\n",
    "    df = df.withColumn(column, when(df[column]==\"\", None).otherwise(df[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Credit _History_Age -> Credic_History_Month\n",
    "### : Convert to Numerical Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example value of Credit_History_Age: 22 Years and 1 Months\n",
    "# We'll convert the each value to numeric like 22*12 + 1, which means the total months of credit history\n",
    "\n",
    "# Divide the each value of \"Credit_History_Age\" by whitespace\n",
    "split_age = split(df[\"Credit_History_Age\"], ' ')\n",
    "\n",
    "# Transform the initial value to total months\n",
    "total_months = (split_age.getItem(0).cast(\"int\") * 12) + split_age.getItem(3).cast(\"int\")\n",
    "\n",
    "# Add new column \"Credit_History_Month\"\n",
    "df = df.withColumn(\"Credit_History_Month\", when(col(\"Credit_History_Age\").isNotNull(), total_months)\n",
    "                   .otherwise(col(\"Credit_History_Age\").cast(DoubleType())))\n",
    "\n",
    "# Remove 'Credit_History_Age'\n",
    "df = df.drop(\"Credit_History_Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update numeric_cols\n",
    "numeric_cols.remove(\"Credit_History_Age\")\n",
    "numeric_cols.append(\"Credit_History_Month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert the data type of all the continuous variables to Double\n",
    "### Since before training, all the variables should be double\n",
    "### Later, we'll do the same task for the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are missing values in the column, the data type cannot be changed.\n",
    "# Therefore, if there are missing values, fill them with an arbitrary value of 100000 and then change the data.\n",
    "# Later, 100000 will be replaced back with missing values.\n",
    "for column in numeric_cols:\n",
    "    df = df.withColumn(column, when(df[column].isNull(), lit(100000)).otherwise(df[column]))\n",
    "\n",
    "# convert all the numeric columns to double\n",
    "for column in numeric_cols:\n",
    "    df = df.withColumn(column, df[column].cast(DoubleType()))\n",
    "    \n",
    "    \n",
    "# Replace 100000 back with missing values\n",
    "for column in numeric_cols:\n",
    "    df = df.withColumn(column, when(df[column] == 100000, None).otherwise(df[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Drop Negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate negative values to null in the case of columns which cannot be negative.\n",
    "def drop_negative(df, column):\n",
    "    df = df.withColumn(column, when(col(column) < 0, None).otherwise(col(column)))\n",
    "    return df\n",
    "\n",
    "columns_to_drop_negative = [\"Age\", \"Num_Bank_Accounts\", \"Num_of_Loan\", \"Delay_from_due_date\", \"Num_of_Delayed_Payment\",\n",
    "                            \"Monthly_Balance\"]\n",
    "\n",
    "for column in columns_to_drop_negative:\n",
    "    df = drop_negative(df, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers of specific columns\n",
    "columns_to_remove_outliers = ['Age', 'Num_Bank_Accounts', 'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan',\n",
    "                               'Num_Credit_Inquiries', 'Num_of_Delayed_Payment',\n",
    "                               'Amount_invested_monthly']\n",
    "\n",
    "for column in columns_to_remove_outliers:\n",
    "    # Calculate Q1, Q3, and IQR\n",
    "    quantiles = df.stat.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define lower and upper bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Convert outliers to NaN\n",
    "    df = df.withColumn(column, \n",
    "                      when((col(column) < lower_bound) | (col(column) > upper_bound), None).otherwise(col(column)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Total_EMI_per_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by Customer_ID and check for cases where the value of Total_EMI_per_month does not duplicate.\n",
    "#If it does not duplicate more than twice, replace it with missing values.\n",
    "\n",
    "window_spec = Window.partitionBy(\"Customer_ID\").orderBy(\"Total_EMI_per_month\")\n",
    "df = df.withColumn(\"count_total_emi\", count(\"Total_EMI_per_month\").over(window_spec))\n",
    "\n",
    "df = df.withColumn(\"Total_EMI_per_month\", \\\n",
    "                   when(col(\"count_total_emi\") <= lit(1), None).otherwise(col(\"Total_EMI_per_month\"))) \\\n",
    "       .drop(\"count_total_emi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual_Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the range of Annual_Income\n",
    "# Removing outliers with IQR deleted some important values.\n",
    "df = df.withColumn(\"Annual_Income\", when(col(\"Annual_Income\") > 180000, None).otherwise(col(\"Annual_Income\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deal with Missing values\n",
    "### - Count Missing Values per Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Customer_ID', 0, 0.0)\n",
      "('Month', 0, 0.0)\n",
      "('Age', 2781, 0.02781)\n",
      "('SSN', 0, 0.0)\n",
      "('Occupation', 7062, 0.07062)\n",
      "('Annual_Income', 999, 0.00999)\n",
      "('Monthly_Inhand_Salary', 15002, 0.15002)\n",
      "('Num_Bank_Accounts', 1336, 0.01336)\n",
      "('Num_Credit_Card', 2271, 0.02271)\n",
      "('Interest_Rate', 2034, 0.02034)\n",
      "('Num_of_Loan', 4348, 0.04348)\n",
      "('Type_of_Loan', 11408, 0.11408)\n",
      "('Delay_from_due_date', 591, 0.00591)\n",
      "('Num_of_Delayed_Payment', 8382, 0.08382)\n",
      "('Changed_Credit_Limit', 2091, 0.02091)\n",
      "('Num_Credit_Inquiries', 3615, 0.03615)\n",
      "('Credit_Mix', 20195, 0.20195)\n",
      "('Outstanding_Debt', 0, 0.0)\n",
      "('Credit_Utilization_Ratio', 0, 0.0)\n",
      "('Payment_of_Min_Amount', 0, 0.0)\n",
      "('Total_EMI_per_month', 114, 0.00114)\n",
      "('Amount_invested_monthly', 14976, 0.14976)\n",
      "('Payment_Behaviour', 7600, 0.076)\n",
      "('Monthly_Balance', 1209, 0.01209)\n",
      "('Credit_Score', 0, 0.0)\n",
      "('Credit_History_Month', 9030, 0.0903)\n"
     ]
    }
   ],
   "source": [
    "def count_missing_values(df):\n",
    "    \"\"\"\n",
    "    A function to calculate the number and proportion of missing values for each column in the given DataFrame\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for column in df.columns:\n",
    "        missing_count = df.filter(col(column).isNull()).count()\n",
    "        missing_rate = missing_count / df.count()\n",
    "        result.append((column, missing_count, missing_rate))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# call the function and print the output\n",
    "missing_values_result = count_missing_values(df)\n",
    "for row in missing_values_result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To fill in missing values in the columns, group by Customer_ID and fill the missing values with the mean or the first value of each group. \n",
    "### This is because the dataset contains monthly financial information for each Customer ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Customer_ID \n",
    "# Since the column will be used to predict other columns' values\n",
    "indexer = StringIndexer(inputCol=\"Customer_ID\", outputCol=\"Customer_ID\" + \"Index\")\n",
    "df = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Case 1: Grouping by Customer_ID based on the column to be predicted.\n",
    "Depending on the type of the column to be predicted (continuous or categorical),\n",
    "the criteria for filling missing values (group mean or first value) varies.\n",
    "\"\"\"\n",
    "\n",
    "def fill_missing_with_mean_or_first(df, column_name):\n",
    "    window_spec = Window().partitionBy(\"Customer_IDIndex\")\n",
    "    return df.withColumn(column_name, when(col(column_name).isNull(),\n",
    "                                           first(column_name, ignorenulls=True).over(window_spec)\n",
    "                                           if column_name in ['Occupation', 'Credit_Mix', 'Payment_Behaviour']\n",
    "                                           else mean(column_name).over(window_spec))\n",
    "                                     .otherwise(col(column_name)))\n",
    "\n",
    "columns_to_fill = ['Age', 'Occupation', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card',\n",
    "                   'Interest_Rate', 'Num_of_Loan', 'Credit_History_Month', 'Num_Credit_Inquiries', 'Credit_Mix',\n",
    "                   'Total_EMI_per_month', 'Payment_Behaviour', 'Num_of_Delayed_Payment', 'Delay_from_due_date',\n",
    "                   'Changed_Credit_Limit']\n",
    "\n",
    "for column in columns_to_fill:\n",
    "    df = fill_missing_with_mean_or_first(df, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Case 2: Filling Missing values of Type_of_Loan (determined by Customer_ID)\n",
    "When grouping by Customer ID, if all values in that group are missing, unify them as \"Not Specified\".\n",
    "If at least one value exists, fill missing values in that group with the first value of the group.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "window_spec = Window().partitionBy(\"Customer_ID\")\n",
    "df = df.withColumn(\"Type_of_Loan\", when(\n",
    "    count(\"Type_of_Loan\").over(window_spec) == 0, \"Not Specified\"\n",
    ").otherwise(\n",
    "    when(col(\"Type_of_Loan\").isNull(), expr(\"first(Type_of_Loan, True)\").over(window_spec))\n",
    "    .otherwise(col(\"Type_of_Loan\"))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Filling missing values using Linear Regression model: Monthly_Balance\n",
    "### Check the RMSE and Determine whether the model performance is Ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of : 151.23850172017958\n"
     ]
    }
   ],
   "source": [
    "def train_regression_model(df, feature_cols, target, split_ratio=0.8, seed=100):\n",
    "    # Feature Vector\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=target)\n",
    "\n",
    "    # Configure Pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "    # Split train and test set\n",
    "    train_data, test_data = df.randomSplit([0.8, 0.2], seed=100)\n",
    "\n",
    "    # train model\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    # Predict test data set\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "    # returned model will be used for future task (filling missing values)\n",
    "    return model, rmse\n",
    "\n",
    "\n",
    "feature_columns = [\"Customer_IDIndex\", \"Annual_Income\", \"Monthly_Inhand_Salary\"]\n",
    "target_column = \"Monthly_Balance\"\n",
    "# Data set without missing values for target column is needed\n",
    "df_no_missing = df.dropna(subset=[target_column])\n",
    "\n",
    "# train data and print the RMSE out\n",
    "model1, rmse1 = train_regression_model(df_no_missing, feature_columns, target_column)\n",
    "print(\"RMSE of :\", rmse1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine if the model is acceptable.\n",
    "\n",
    "An RMSE of 151 implies an error of approximately 12 between the predicted and actual values, which seems to be an acceptable level of error.\n",
    "\n",
    "Therefore, we proceed with the missing value handling task using this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable, input variables, and join columns\n",
    "target_variable = \"Monthly_Balance\"\n",
    "join_columns = [\"Customer_IDIndex\", \"Month\", \"Annual_Income\", \"Monthly_Inhand_Salary\"]\n",
    "\n",
    "\n",
    "# Create an instance of the MissingValuePredictor with the specified model\n",
    "missing_value_predictor = MissingValuePredictor(target_col=target_variable,\n",
    "                                                join_cols=join_columns,\n",
    "                                                model=model1)\n",
    "\n",
    "# Apply the transformation\n",
    "df = missing_value_predictor.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Filling missing values using specific prediction model: Amount_invested_monthly\n",
    "### Check the RMSE and Determine whether the model performance is Ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of : 95.787619699179\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [\"Customer_IDIndex\", \"Annual_Income\", \"Monthly_Inhand_Salary\"]\n",
    "target_column = \"Amount_invested_monthly\"\n",
    "# Data set without missing values for target column is needed\n",
    "df_no_missing = df.dropna(subset=[target_column])\n",
    "\n",
    "# train data and print the RMSE out\n",
    "model2, rmse2 = train_regression_model(df_no_missing, feature_columns, target_column)\n",
    "print(\"RMSE of :\", rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine if the model is acceptable.\n",
    "\n",
    "An RMSE of 95 implies an error of approximately 9~10 between the predicted and actual values, which seems to be an acceptable level of error.\n",
    "\n",
    "Therefore, we proceed with the missing value handling task using this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable, input variables, and join columns\n",
    "target_variable = \"Amount_invested_monthly\"\n",
    "join_columns = [\"Customer_IDIndex\", \"Month\", \"Annual_Income\", \"Monthly_Inhand_Salary\"]\n",
    "\n",
    "\n",
    "# Create an instance of the MissingValuePredictor with the specified model\n",
    "missing_value_predictor = MissingValuePredictor(target_col=target_variable,\n",
    "                                                join_cols=join_columns,\n",
    "                                                model=model2)\n",
    "\n",
    "# Apply the transformation\n",
    "df = missing_value_predictor.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - After filling all the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether the number of rows is maintained\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Customer_IDIndex', 0, 0.0)\n",
      "('Month', 0, 0.0)\n",
      "('Annual_Income', 0, 0.0)\n",
      "('Monthly_Inhand_Salary', 0, 0.0)\n",
      "('Customer_ID', 0, 0.0)\n",
      "('Age', 0, 0.0)\n",
      "('SSN', 0, 0.0)\n",
      "('Occupation', 0, 0.0)\n",
      "('Num_Bank_Accounts', 0, 0.0)\n",
      "('Num_Credit_Card', 0, 0.0)\n",
      "('Interest_Rate', 0, 0.0)\n",
      "('Num_of_Loan', 0, 0.0)\n",
      "('Type_of_Loan', 0, 0.0)\n",
      "('Delay_from_due_date', 0, 0.0)\n",
      "('Num_of_Delayed_Payment', 0, 0.0)\n",
      "('Changed_Credit_Limit', 0, 0.0)\n",
      "('Num_Credit_Inquiries', 0, 0.0)\n",
      "('Credit_Mix', 0, 0.0)\n",
      "('Outstanding_Debt', 0, 0.0)\n",
      "('Credit_Utilization_Ratio', 0, 0.0)\n",
      "('Payment_of_Min_Amount', 0, 0.0)\n",
      "('Total_EMI_per_month', 0, 0.0)\n",
      "('Amount_invested_monthly', 0, 0.0)\n",
      "('Payment_Behaviour', 0, 0.0)\n",
      "('Monthly_Balance', 0, 0.0)\n",
      "('Credit_Score', 0, 0.0)\n",
      "('Credit_History_Month', 0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# Now, there must not be any missing values\n",
    "\n",
    "missing_values_result = count_missing_values(df)\n",
    "for row in missing_values_result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding: Categorical --> Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month mapping: Since the column is ordinal\n",
    "month_mapping = {\n",
    "    \"January\": 1,\n",
    "    \"February\": 2,\n",
    "    \"March\": 3,\n",
    "    \"April\": 4,\n",
    "    \"May\": 5,\n",
    "    \"June\": 6,\n",
    "    \"July\": 7,\n",
    "    \"August\": 8,\n",
    "    \"September\": 9,\n",
    "    \"October\": 10,\n",
    "    \"November\": 11,\n",
    "    \"December\": 12    \n",
    "}\n",
    "\n",
    "# Iterate over the month_mapping and update the \"Month\" column in df\n",
    "for month_name, numeric_value in month_mapping.items():\n",
    "    df = df.withColumn(\"Month\", when(df[\"Month\"] == month_name, numeric_value).otherwise(df[\"Month\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update categorical_cols\n",
    "categorical_cols = ['SSN', 'Occupation',\n",
    "                    'Credit_Mix', 'Type_of_Loan', 'Payment_of_Min_Amount',\n",
    "                    'Payment_Behaviour']\n",
    "\n",
    "#Add indexed categorical columns to the original DataFrame df.\n",
    "#The name of the new DataFrame with the applied changes is indexed.\n",
    "\n",
    "indexed = df\n",
    "for col in categorical_cols:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=col + \"_Index\")\n",
    "    indexed = indexer.fit(indexed).transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Target|\n",
      "+------+\n",
      "|   0.0|\n",
      "|   1.0|\n",
      "|   2.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Target Encoding\n",
    "# Credit_Score --> Target\n",
    "indexer = StringIndexer(inputCol=\"Credit_Score\", outputCol=\"Target\")\n",
    "indexed = indexer.fit(indexed).transform(indexed)\n",
    "indexed.select('Target').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert all to double\n",
    "\n",
    "### In PySpark, unlike Scikit-Learn, it is necessary to convert the data type of all variables to Double Type as independent variables before the implementation stage of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical\n",
    "for col in categorical_cols:\n",
    "    indexed = indexed.withColumn(col, indexed[col].cast(DoubleType()))\n",
    "# Month\n",
    "indexed = indexed.withColumn(\"Month\", indexed[\"Month\"].cast(DoubleType()))\n",
    "# Rename the column\n",
    "indexed = indexed.withColumnRenamed(\"Month\", \"Month_Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the columns which are not indexed (original categorical columns)\n",
    "exclude = ['Customer_ID', 'SSN', 'Occupation', 'Credit_Mix', 'Type_of_Loan',\n",
    "                  'Payment_of_Min_Amount','Payment_Behaviour', 'Credit_Score', 'Amount_invested_monthly']\n",
    "\n",
    "selected_cols = [col for col in indexed.columns if col not in exclude]\n",
    "indexed = indexed.select(selected_cols)\n",
    "\n",
    "# update numeric_cols\n",
    "numeric_cols.remove(\"Amount_invested_monthly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Customer_IDIndex: double (nullable = false)\n",
      " |-- Month_Index: double (nullable = true)\n",
      " |-- Annual_Income: double (nullable = true)\n",
      " |-- Monthly_Inhand_Salary: double (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Num_Bank_Accounts: double (nullable = true)\n",
      " |-- Num_Credit_Card: double (nullable = true)\n",
      " |-- Interest_Rate: double (nullable = true)\n",
      " |-- Num_of_Loan: double (nullable = true)\n",
      " |-- Delay_from_due_date: double (nullable = true)\n",
      " |-- Num_of_Delayed_Payment: double (nullable = true)\n",
      " |-- Changed_Credit_Limit: double (nullable = true)\n",
      " |-- Num_Credit_Inquiries: double (nullable = true)\n",
      " |-- Outstanding_Debt: double (nullable = true)\n",
      " |-- Credit_Utilization_Ratio: double (nullable = true)\n",
      " |-- Total_EMI_per_month: double (nullable = true)\n",
      " |-- Monthly_Balance: double (nullable = true)\n",
      " |-- Credit_History_Month: double (nullable = true)\n",
      " |-- SSN_Index: double (nullable = false)\n",
      " |-- Occupation_Index: double (nullable = false)\n",
      " |-- Credit_Mix_Index: double (nullable = false)\n",
      " |-- Type_of_Loan_Index: double (nullable = false)\n",
      " |-- Payment_of_Min_Amount_Index: double (nullable = false)\n",
      " |-- Payment_Behaviour_Index: double (nullable = false)\n",
      " |-- Target: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### check the data types\n",
    "indexed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of classes of label (Credit_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# count per class\n",
    "counts = indexed.groupBy(\"Target\").count().orderBy(\"Target\").collect()\n",
    "\n",
    "# visualization\n",
    "credit_scores = [row[\"Target\"] for row in counts]\n",
    "counts = [row[\"count\"] for row in counts]\n",
    "\n",
    "plt.bar(credit_scores, counts, color='skyblue')\n",
    "plt.xlabel('Credit Score Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Credit Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Target|count|\n",
      "+------+-----+\n",
      "|   0.0|53174|\n",
      "|   1.0|28998|\n",
      "|   2.0|17828|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed.groupBy(\"Target\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the visualization and table above, it can be observed that the classes of the target variable (\"Credit_Score\") are imbalanced. Such imbalance can lead to reduced accuracy in model implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data distribution of the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from pylab import rcParams\n",
    "\n",
    "df_pd = df.toPandas()\n",
    "rcParams[\"figure.figsize\"]=(20,22)\n",
    "df.hist()\n",
    "plt.grid()\n",
    "\"\"\"\n",
    "\n",
    "### Please refer to the visualization.ipynb for this code block\n",
    "# toPandas() method doesn't work due to the PySpark version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-Defined Transformer: Create New Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a process of creating a new feature.\n",
    "### The new feature is obtained through the ratio of two existing columns.\n",
    "### The following formula is used to generate the feature:\n",
    "### inputCol1 / (inputCol2 + epsilon)\n",
    "## The transformer below includes a parameter that determines whether to apply the new feature to the DataFrame, i.e., it determines if it's a variable to be used in Preciction Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import Param, Params\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# When 'useNewFeature' is True, new feature will be added to the data frame\n",
    "class NewFeatureTransformer(Transformer, Params):\n",
    "    \"\"\"\n",
    "    Transformer to add a new feature to the DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputCol1, inputCol2, outputCol, useNewFeature = True):\n",
    "        \"\"\"\n",
    "        Initializes the NewFeatureTransformer with input and output column names\n",
    "        and a flag indicating whether to use the new feature or not.\n",
    "        \"\"\"\n",
    "        super(NewFeatureTransformer, self).__init__()\n",
    "        self.inputCol1 = inputCol1\n",
    "        self.inputCol2 = inputCol2\n",
    "        self.outputCol = outputCol\n",
    "        self.useNewFeature = Param(self, \"useNewFeature\", \"Flag to indicate whether to use the new feature or not\")\n",
    "        self.useNewFeature.default = True\n",
    "        self.setUseNewFeature(useNewFeature)\n",
    "\n",
    "    def setUseNewFeature(self, value: bool):\n",
    "        \"\"\"\n",
    "        Sets the flag indicating whether to use the new feature or not.\n",
    "        \"\"\"\n",
    "        self._paramMap[self.useNewFeature] = value\n",
    "        return self\n",
    "\n",
    "    def getUseNewFeature(self):\n",
    "        \"\"\"\n",
    "        Gets the flag indicating whether to use the new feature or not.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.useNewFeature)\n",
    "\n",
    "    def _transform(self, df) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Adds a new feature to the DataFrame based on the specified input columns,\n",
    "        and returns the DataFrame with or without the new feature based on the flag.\n",
    "        \"\"\"\n",
    "        if self.getUseNewFeature():\n",
    "            epsilon = 1e-4  # To avoid division by zero\n",
    "            # Add a new feature based on the input column\n",
    "            df = df.withColumn(self.outputCol, col(self.inputCol1) / (col(self.inputCol2)+epsilon))\n",
    "        else:\n",
    "            # Drop the output column if not using the new feature\n",
    "            df = df.drop(self.outputCol)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st New Feature: Financial_Stability\n",
    "new_feature_transformer = NewFeatureTransformer(inputCol1=\"Monthly_Inhand_Salary\",\n",
    "                                                inputCol2=\"Monthly_Balance\",\n",
    "                                                outputCol=\"Financial_Stability\", useNewFeature=True)\n",
    "indexed = new_feature_transformer.transform(indexed)\n",
    "\n",
    "# 2nd New Feature: Loan_Replacement_Capacity\n",
    "new_feature_transformer = NewFeatureTransformer(inputCol1=\"Total_EMI_per_month\",\n",
    "                                                inputCol2=\"Monthly_Inhand_Salary\",\n",
    "                                                outputCol=\"Loan_Replacement_Capacity\", useNewFeature=True)\n",
    "indexed = new_feature_transformer.transform(indexed)\n",
    "\n",
    "# 3rd New Feature: Income_To_Debt_Ratio\n",
    "new_feature_transformer = NewFeatureTransformer(inputCol1=\"Annual_Income\",\n",
    "                                                inputCol2=\"Outstanding_Debt\",\n",
    "                                                outputCol=\"Income_To_Debt_Ratio\", useNewFeature=True)\n",
    "indexed = new_feature_transformer.transform(indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this case, we added all the new features to the dataframe 'indexed'.\n",
    "### But we can just select specific features to use among three by adjusting the value of parameter 'useNewFeature'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Customer_IDIndex: double (nullable = false)\n",
      " |-- Month_Index: double (nullable = true)\n",
      " |-- Annual_Income: double (nullable = true)\n",
      " |-- Monthly_Inhand_Salary: double (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Num_Bank_Accounts: double (nullable = true)\n",
      " |-- Num_Credit_Card: double (nullable = true)\n",
      " |-- Interest_Rate: double (nullable = true)\n",
      " |-- Num_of_Loan: double (nullable = true)\n",
      " |-- Delay_from_due_date: double (nullable = true)\n",
      " |-- Num_of_Delayed_Payment: double (nullable = true)\n",
      " |-- Changed_Credit_Limit: double (nullable = true)\n",
      " |-- Num_Credit_Inquiries: double (nullable = true)\n",
      " |-- Outstanding_Debt: double (nullable = true)\n",
      " |-- Credit_Utilization_Ratio: double (nullable = true)\n",
      " |-- Total_EMI_per_month: double (nullable = true)\n",
      " |-- Monthly_Balance: double (nullable = true)\n",
      " |-- Credit_History_Month: double (nullable = true)\n",
      " |-- SSN_Index: double (nullable = false)\n",
      " |-- Occupation_Index: double (nullable = false)\n",
      " |-- Credit_Mix_Index: double (nullable = false)\n",
      " |-- Type_of_Loan_Index: double (nullable = false)\n",
      " |-- Payment_of_Min_Amount_Index: double (nullable = false)\n",
      " |-- Payment_Behaviour_Index: double (nullable = false)\n",
      " |-- Target: double (nullable = false)\n",
      " |-- Financial_Stability: double (nullable = true)\n",
      " |-- Loan_Replacement_Capacity: double (nullable = true)\n",
      " |-- Income_To_Debt_Ratio: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can see that 3 new features are added to dataframe indexed\n",
    "indexed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Customer_IDIndex', 0, 0.0)\n",
      "('Month_Index', 0, 0.0)\n",
      "('Annual_Income', 0, 0.0)\n",
      "('Monthly_Inhand_Salary', 0, 0.0)\n",
      "('Age', 0, 0.0)\n",
      "('Num_Bank_Accounts', 0, 0.0)\n",
      "('Num_Credit_Card', 0, 0.0)\n",
      "('Interest_Rate', 0, 0.0)\n",
      "('Num_of_Loan', 0, 0.0)\n",
      "('Delay_from_due_date', 0, 0.0)\n",
      "('Num_of_Delayed_Payment', 0, 0.0)\n",
      "('Changed_Credit_Limit', 0, 0.0)\n",
      "('Num_Credit_Inquiries', 0, 0.0)\n",
      "('Outstanding_Debt', 0, 0.0)\n",
      "('Credit_Utilization_Ratio', 0, 0.0)\n",
      "('Total_EMI_per_month', 0, 0.0)\n",
      "('Monthly_Balance', 0, 0.0)\n",
      "('Credit_History_Month', 0, 0.0)\n",
      "('SSN_Index', 0, 0.0)\n",
      "('Occupation_Index', 0, 0.0)\n",
      "('Credit_Mix_Index', 0, 0.0)\n",
      "('Type_of_Loan_Index', 0, 0.0)\n",
      "('Payment_of_Min_Amount_Index', 0, 0.0)\n",
      "('Payment_Behaviour_Index', 0, 0.0)\n",
      "('Target', 0, 0.0)\n",
      "('Financial_Stability', 0, 0.0)\n",
      "('Loan_Replacement_Capacity', 0, 0.0)\n",
      "('Income_To_Debt_Ratio', 0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# check the missing values after adding new features\n",
    "missing_values_result = count_missing_values(indexed)\n",
    "for row in missing_values_result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update numeric_cols\n",
    "numeric_cols.append(\"Financial_Stability\")\n",
    "numeric_cols.append(\"Loan_Replacement_Capacity\")\n",
    "numeric_cols.append(\"Income_To_Debt_Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering was conducted based on the insights of banking professionals and background knowledge.\n",
    "\n",
    "#### Customer_ID\n",
    "#### : The overall dataset consists of monthly financial information for each customer. Since most columns are associated with Customer_ID, it is adopted as an input variable.\n",
    "\n",
    "#### Month\n",
    "#### : The overall dataset consists of monthly financial information for each customer. Therefore, it is adopted as an independent variable.\n",
    "\n",
    "#### Annual_Income\n",
    "#### : Annual_Income is a factor that influences credit rating classification. Therefore, it is adopted as an independent variable.\n",
    "\n",
    "#### Monthly_Inhand_Salary\n",
    "#### : Monthly_Inhand_Salary is a factor that influences credit rating classification. However, since it holds almost identical informational value to Annual_Income, it is excluded from the independent variables.\n",
    "\n",
    "#### SSN\n",
    "#### : SSN includes customer's residency information. In some banks, this information is used in credit rating decisions.\n",
    "\n",
    "#### Occupation\n",
    "#### : Customer's occupation information is a significant factor affecting credit ratings. Therefore, it is adopted as an independent variable.\n",
    "\n",
    "#### Age\n",
    "#### : Age is a factor that influences credit rating classification. Therefore, it is adopted as an independent variable.\n",
    "\n",
    "#### Interest_Rate, Num_of_Loan, Delay_from_due_date, Num_of_Delayed_Payment, Outstanding_Debt\n",
    "#### : These pieces of information are crucial factors in credit rating decisions.\n",
    "\n",
    "#### Additionally, the remaining columns also have an impact on credit rating decisions and thus can be adopted as independent variables.\n",
    "\n",
    "#### Changed_Credit_Limit, Num_Credit_Inquiries, Credit_Utilization_Ratio, Total_EMI_per_month, Monthly_Balance,  Credit_History_Month, Credit_Mix, Type_of_Loan, Payment_of_Min_Amount, Payment_Behaviour\n",
    "\n",
    "### In conclusion, except for ID and Name, most columns have been selected as independent variables for predicting Credit_Score. Concerns about overfitting due to the curse of dimensionality will be addressed by comparing the accuracy difference between training and test sets after executing the actual model. Consideration of the relationship between independent variables has generally been excluded as multicollinearity has minimal impact in classification problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline: Scale + Feature Engineering + Repartitioning\n",
    "### Repartitioning in PySpark refers to the process of splitting data into multiple partitions. Each partition enables parallel processing in a distributed environment. Repartitioning is particularly important when dealing with large-scale datasets as each partition can be processed independently in a distributed environment, thereby improving processing speed. Additionally, adjusting the number of partitions allows for optimization of cluster resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# We can modify the list 'to_remove' as the task of Feature Engineering\n",
    "feature_list = indexed.columns\n",
    "to_remove = ['Monthly_Inhand_Salary', 'Target']\n",
    "\n",
    "# SparkDFPipleline is defined at the beginning\n",
    "pipeline = SparkDFPipeline(case=0)\n",
    "preprocessed_data  = pipeline.transform(indexed, feature_list, to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Sampling (8:2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fractions for each stratum\n",
    "fractions = preprocessed_data.groupBy('Target').count().withColumn('fraction',\n",
    "                                                                   F.lit(0.8)).select('Target', 'fraction').collect()\n",
    "\n",
    "# Convert the result to a dictionary\n",
    "fractions_dict = {row['Target']: row['fraction'] for row in fractions}\n",
    "\n",
    "# Perform stratified sampling using sampleBy\n",
    "df_train = preprocessed_data.sampleBy('Target', fractions_dict, seed=100)\n",
    "df_test = preprocessed_data.subtract(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation\n",
    "# : Random Forest, Decision Tree, Logistic Regression(Multinomial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.6662706204584964\n",
      "test accuracy: 0.6700593723494487\n"
     ]
    }
   ],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"Target\", featuresCol=\"scaled_features\", numTrees=10)\n",
    "rf_model = rf.fit(df_train)\n",
    "prediction = rf_model.transform(df_train)\n",
    "\n",
    "Eval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"Target\")\n",
    "print(f'training accuracy: {Eval.evaluate(prediction)}')\n",
    "\n",
    "prediction_test = rf_model.transform(df_test)\n",
    "Eval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"Target\")\n",
    "print(f'test accuracy: {Eval.evaluate(prediction_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"Target\", featuresCol=\"scaled_features\", numTrees=10)\n",
    "rf_model = rf.fit(df_train)\n",
    "\n",
    "evaluation_results_RF = evaluate_model(rf_model, df_test)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"{'Metric':<20}{'RF Model':<20}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for metric in evaluation_results_RF.keys():\n",
    "    default_value = evaluation_results_RF[metric]\n",
    "    print(f\"{metric:<20}{default_value:<20.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tunning (Hyper parameter tunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline with RandomForest as the estimator\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "            .addGrid(rf.maxDepth, [5, 6, 8, 10]) \\\n",
    "            .\n",
    "            .build()\n",
    "\n",
    "# Define the cross-validator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Fit the pipeline with cross-validation to find the best model\n",
    "cv_model = crossval.fit(df_train)\n",
    "\n",
    "# Get the best model from cross-validation\n",
    "best_rf_model = cv_model.bestModel\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "predictions_test = best_rf_model.transform(df_test)\n",
    "\n",
    "# Evaluate the predictions on the test set\n",
    "test_accuracy = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(predictions_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Get the best parameters\n",
    "best_numTrees = best_rf_model.getOrDefault('numTrees')\n",
    "best_maxDepth = best_rf_model.getOrDefault('maxDepth')\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best numTrees:\", best_numTrees)\n",
    "print(\"Best maxDepth:\", best_maxDepth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_RF = evaluate_model(best_rf_model, df_test)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"{'Metric':<20}{'RF Model':<20}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for metric in evaluation_results_RF.keys():\n",
    "    default_value = evaluation_results_RF[metric]\n",
    "    print(f\"{metric:<20}{default_value:<20.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DecisionTree model\n",
    "dt = DecisionTreeClassifier(labelCol=\"Target\", featuresCol=\"scaled_features\")\n",
    "\n",
    "# Train the DecisionTree model\n",
    "dt_model = dt.fit(df_train)\n",
    "\n",
    "evaluation_results_DT = evaluate_model(dt_model, df_test)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"{'Metric':<20}{'DT Model':<20}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for metric in evaluation_results_DT.keys():\n",
    "    default_value = evaluation_results_DT[metric]\n",
    "    print(f\"{metric:<20}{default_value:<20.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important:\n",
    "## We obtained the best parameters through fine-tuning code blocks previously, but after restarting the kernel, the output disappeared. \n",
    "## Since it takes over 3 hours to execute the code block and obtain the output, we proceeded to run the evaluator based on the previously obtained best parameters.\n",
    "### Best maxDepth:  15\n",
    "### Best maxBins:  60\n",
    "### Best minInstancesPerNode:  1\n",
    "### Best minInfoGain:  0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with the best parameters\n",
    "best_dt = DecisionTreeClassifier(labelCol=\"Target\", featuresCol=\"scaled_features\",\n",
    "                                 maxDepth=15, maxBins=60, minInstancesPerNode=1, minInfoGain=0.0)\n",
    "evaluation_results_DT = evaluate_model(best_dt, df_test)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"{'Metric':<20}{'DT Model':<20}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for metric in evaluation_results_DT.keys():\n",
    "    default_value = evaluation_results_DT[metric]\n",
    "    print(f\"{metric:<20}{default_value:<20.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tunning (Hyper parameter tunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with the defined stages\n",
    "pipeline = Pipeline(stages=[dt])\n",
    "\n",
    "# Create a grid of parameters to search over\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15, 20]) \\\n",
    "    .addGrid(dt.maxBins, [20, 32, 40, 60, 80]) \\\n",
    "    .addGrid(dt.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .addGrid(dt.minInfoGain, [0.0, 0.1, 0.2]) \\\n",
    "    .build()\n",
    "\n",
    "# Set up the evaluator for multi-class classification\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Create a cross-validator\n",
    "cross_validator = CrossValidator(estimator=dt,\n",
    "                                estimatorParamMaps=param_grid,\n",
    "                                evaluator=evaluator,\n",
    "                                numFolds=5)  # You can adjust the number of folds as needed\n",
    "\n",
    "# Fit the cross-validator to the training data\n",
    "cv_model = cross_validator.fit(df_train)\n",
    "\n",
    "# Get the best model from the cross-validation\n",
    "best_dt_model = cv_model.bestModel\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "prediction_test_dt = best_dt_model.transform(df_test)\n",
    "\n",
    "# Evaluate the test accuracy using the best model\n",
    "test_accuracy_dt = evaluator.evaluate(prediction_test_dt)\n",
    "print(f'Test accuracy (Decision Tree): {test_accuracy_dt}')\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best maxDepth: \", best_dt_model._java_obj.getMaxDepth())\n",
    "print(\"Best maxBins: \", best_dt_model._java_obj.getMaxBins())\n",
    "print(\"Best minInstancesPerNode: \", best_dt_model._java_obj.getMinInstancesPerNode())\n",
    "print(\"Best minInfoGain: \", best_dt_model._java_obj.getMinInfoGain())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_DT = evaluate_model(best_dt_model, df_test)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"{'Metric':<20}{'DT Model':<20}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for metric in evaluation_results_DT.keys():\n",
    "    default_value = evaluation_results_DT[metric]\n",
    "    print(f\"{metric:<20}{default_value:<20.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (Logistic Regression): 0.6466975999599784\n",
      "Test accuracy (Logistic Regression): 0.6528463802823928\n"
     ]
    }
   ],
   "source": [
    "# Define the Logistic Regression model for multi-class classification\n",
    "lr = LogisticRegression(labelCol=\"Target\", featuresCol=\"scaled_features\", maxIter=10, family=\"multinomial\")\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "lr_model = lr.fit(df_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "prediction_train_lr = lr_model.transform(df_train)\n",
    "\n",
    "# Evaluate the training accuracy for multi-class classification\n",
    "multiEval_train_lr = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") \\\n",
    "    .setPredictionCol(\"prediction\").setLabelCol(\"Target\")\n",
    "training_accuracy_lr = multiEval_train_lr.evaluate(prediction_train_lr)\n",
    "print(f'Training accuracy (Logistic Regression): {training_accuracy_lr}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "prediction_test_lr = lr_model.transform(df_test)\n",
    "\n",
    "# Evaluate the test accuracy for multi-class classification\n",
    "multiEval_test_lr = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") \\\n",
    "    .setPredictionCol(\"prediction\").setLabelCol(\"Target\")\n",
    "test_accuracy_lr = multiEval_test_lr.evaluate(prediction_test_lr)\n",
    "print(f'Test accuracy (Logistic Regression): {test_accuracy_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tunning (Hyper parameter tunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with the defined stages\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# Create a grid of parameters to search over\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.maxIter, [10, 100, 1000]) \\\n",
    "    .build()\n",
    "\n",
    "# Set up the evaluator for multi-class classification\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Create a cross-validator\n",
    "cross_validator = CrossValidator(estimator=pipeline,\n",
    "                                estimatorParamMaps=param_grid,\n",
    "                                evaluator=evaluator,\n",
    "                                numFolds=5)  # You can adjust the number of folds as needed\n",
    "\n",
    "# Fit the cross-validator to the training data\n",
    "cv_model = cross_validator.fit(df_train)\n",
    "\n",
    "# Get the best model from the cross-validation\n",
    "best_lr_model = cv_model.bestModel.stages[0]\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "prediction_test_lr = best_lr_model.transform(df_test)\n",
    "\n",
    "# Evaluate the test accuracy using the best model\n",
    "test_accuracy_lr = evaluator.evaluate(prediction_test_lr)\n",
    "print(f'Test accuracy (Logistic Regression): {test_accuracy_lr}')\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best regParam: \", best_lr_model.getRegParam())\n",
    "print(\"Best maxIter: \", best_lr_model.getMaxIter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_LR = evaluate_model(best_lr_model, df_test)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"{'Metric':<20}{'LR Model':<20}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for metric in evaluation_results_LR.keys():\n",
    "    default_value = evaluation_results_LR[metric]\n",
    "    print(f\"{metric:<20}{default_value:<20.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------Report---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curse of Demiensionality?\n",
    "## It appears that there is no overfitting observed as the accuracy on the test set is slightly higher than that on the training set for all models. This suggests that the curse of dimensionality has been avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison among Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between Spark MLlib and Scikit-Learn (e.g., their pros/cons or similarity/difference). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark MLlib is designed for big data processing and can efficiently handle large datasets distributed across multiple nodes in a cluster. It leverages the power of distributed computing, making it suitable for parallel processing tasks, which can lead to faster training times. However, setting up and configuring a Spark cluster is complex, especially for users unfamiliar with distributed computing concepts. While Spark MLlib provides a wide range of machine-learning algorithms, it does not offer the same breadth and depth of algorithms as Scikit-Learn, which has been developed for a longer time.\n",
    "\n",
    "### On the other hand, Scikit-Learn has a user-friendly and intuitive API, making it easy for beginners to get started with machine learning tasks. However, its algorithms may consume a significant amount of memory, especially when dealing with large datasets, which can be a limitation for machines with limited resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE sampling and the Min-Max scaler applied in the previous assignment are determined to improve model accuracy.But these can't be easily implemented in PySpark. It posed challenges being limited to PySpark and not attempting any conversions from pandas data to PySpark. Without implementing  SMOTE and Min-Max scaler (but used Standard Scaler in this assignment), we got relatively low accuracy.\n",
    "### Even though these challenges, we maximized the use of PySpark by implementing everything solely using it, starting from the preprocessing stage. This ensured that we focused on the main theme of this task, which is maximizing the utilization of PySpark."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2289007,
     "sourceId": 3846912,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
