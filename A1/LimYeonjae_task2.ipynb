{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE5Lbz5Ynfqv"
      },
      "source": [
        "# CSCI316 Individual Assignment 1 - Task 2\n",
        "\n",
        "This code include:\n",
        "* Preprocessing\n",
        "* Binning\n",
        "* Functions for Information Gain\n",
        "* Implementation of Decision Tree\n",
        "* Random Sampling\n",
        "* View Accuracy\n",
        "* Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "3H3bYwON7z01"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8PycCkYniY8"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "Di3CRUwm78z6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data.csv\", delimiter=';')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DxXBxv2nkXh"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "5PVx17PW7-D4"
      },
      "outputs": [],
      "source": [
        "# Label Encoding\n",
        "df['Target'] = df['Target'].map({'Graduate': 0, 'Enrolled': 1,'Dropout': 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "JJErMxuu8GAQ"
      },
      "outputs": [],
      "source": [
        "# Binning\n",
        "# Only performed on features with many continuous values (numerous unique values).\n",
        "pre_qualification = df['Previous qualification (grade)']\n",
        "bins = [0, 100, 150, 200]\n",
        "labels = ['0-100', '101-150', '151-200']\n",
        "df['Pre_qualification_group'] = pd.cut(pre_qualification, bins=bins, labels=labels, right=False)\n",
        "df['Pre_qualification_group'] = df['Pre_qualification_group'].map({'0-100': 0, '101-150': 1,'151-200': 2})\n",
        "\n",
        "addmission_grade = df['Admission grade']\n",
        "bins = [0, 100, 150, 200]\n",
        "labels = ['0-100', '101-150', '151-200']\n",
        "df['Addmission_grade_group'] = pd.cut(addmission_grade, bins=bins, labels=labels, right=False)\n",
        "df['Addmission_grade_group'] = df['Addmission_grade_group'].map({'0-100': 0, '101-150': 1,'151-200': 2})\n",
        "\n",
        "ages = df['Age at enrollment']\n",
        "bins = [0, 20, 30, 40, 50, 100]\n",
        "labels = ['0-20', '21-30', '31-40', '41-50', '51-100']\n",
        "df['Age_group'] = pd.cut(ages, bins=bins, labels=labels, right=False)\n",
        "df['Age_group'] = df['Age_group'].map({'0-20': 0, '21-30': 1,'31-40': 2, '41-50': 3, '51-100': 4})\n",
        "\n",
        "# Curricular units 1st sem (grade)\n",
        "grade1 = df['Curricular units 1st sem (grade)']\n",
        "bins = [0, 5, 10, 15, 20]\n",
        "labels = ['0-5', '5-10', '10-15', '15-20']\n",
        "df['grade1_group'] = pd.cut(grade1, bins=bins, labels=labels, right=False)\n",
        "df['grade1_group'] = df['grade1_group'].map({'0-5': 0, '5-10': 1,'10-15': 2,'15-20': 3})\n",
        "\n",
        "# Curricular units 2nd sem (grade)\n",
        "grade2 = df['Curricular units 2nd sem (grade)']\n",
        "bins = [0, 5, 10, 15, 20]\n",
        "labels = ['0-5', '5-10', '10-15', '15-20']\n",
        "df['grade2_group'] = pd.cut(grade2, bins=bins, labels=labels, right=False)\n",
        "df['grade2_group'] = df['grade2_group'].map({'0-5': 0, '5-10': 1,'10-15': 2,'15-20': 3})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HGTwYSAnnhG"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrMnthzfmwo8",
        "outputId": "c2a4ef58-481b-4232-eebb-056af680f79f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "<(feature)>\n",
            "Chi-square: []\n",
            "P-value: 0.0007579\n",
            "\n",
            "<(feature)>\n",
            "Chi-square: []\n",
            "P-value: 5e-07\n",
            "\n",
            "<(feature)>\n",
            "Chi-square: []\n",
            "P-value: 0.0\n",
            "\n",
            "<(feature)>\n",
            "Chi-square: []\n",
            "P-value: 0.0\n",
            "\n",
            "<(feature)>\n",
            "Chi-square: []\n",
            "P-value: 0.0\n",
            "\n",
            "<(feature)>\n",
            "Chi-square: []\n",
            "P-value: 0.0\n",
            "\n",
            "<(feature)>\n",
            "Chi-square: []\n",
            "P-value: 0.0014575\n",
            "\n",
            "<(feature)>\n",
            "Chi-square: []\n",
            "P-value: 0.0\n",
            "\n",
            "<(feature)>\n",
            "Chi-square: []\n",
            "P-value: 1.9e-06\n",
            "\n",
            "<(feature)>\n",
            "Chi-square: []\n",
            "P-value: 0.0\n"
          ]
        }
      ],
      "source": [
        "import scipy.stats as stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Done Chi_square_test to select only significant features\n",
        "def chi_square_test(data, feature, target, significant_features):\n",
        "  contingency_table = pd.crosstab(data[feature], data[target])\n",
        "  observed = contingency_table.values\n",
        "  col_totals = observed.sum(axis=0)\n",
        "  row_totals = observed.sum(axis=1)\n",
        "  expected = np.outer(row_totals, col_totals)/data.shape[0]\n",
        "\n",
        "  epsilone = 1e-10\n",
        "  chi_squared_stat = ((observed - expected) ** 2/ expected).sum().sum()\n",
        "  degrees_of_freedom = (observed.shape[0] - 1) * (observed.shape[1] - 1) + epsilone\n",
        "  chi_squared_stat /= degrees_of_freedom\n",
        "\n",
        "  p_value = 1 - stats.chi2.cdf(chi_squared_stat, degrees_of_freedom)\n",
        "\n",
        "  if p_value < 0.05:\n",
        "    print(f'\\n<(feature)>')\n",
        "    print('Chi-square: []'.format(round(chi_squared_stat, 2)))\n",
        "    print('P-value: {}'.format(round (p_value, 7)))\n",
        "    significant_features.append(feature)\n",
        "\n",
        "significant_features = []\n",
        "\n",
        "for cols in df.columns[:-1]:\n",
        "  chi_square_test(df, cols, 'Target', significant_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czOBEZDymzVl",
        "outputId": "ff4d2da1-54a4-4bee-c56e-5d5ed098e8ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Daytime/evening attendance\\t',\n",
              " 'Displaced',\n",
              " 'Debtor',\n",
              " 'Tuition fees up to date',\n",
              " 'Gender',\n",
              " 'Scholarship holder',\n",
              " 'Curricular units 2nd sem (approved)',\n",
              " 'Target',\n",
              " 'Age_group',\n",
              " 'grade1_group']"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "significant_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "TXv_pM1N8OQb"
      },
      "outputs": [],
      "source": [
        "# Since above significant_features includes 'Target',\n",
        "# created the list of feature without 'Target'\n",
        "names = ['Daytime/evening attendance\\t','Displaced','Debtor',\n",
        "         'Tuition fees up to date','Gender','Scholarship holder',\n",
        "         'Curricular units 2nd sem (approved)','Age_group','grade1_group']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytYuQtyzndYY"
      },
      "source": [
        "Utilizing only significant features offers benefits such as simplification and interpretability of models, reduction of uncertainty, better generalization, and potentially improved performance. By eliminating unnecessary information, models can make more informed decisions and produce more reliable predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuzsAKgBnsCu"
      },
      "source": [
        "## Implement Decision Tree Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "TZRHSuAn9VBm"
      },
      "outputs": [],
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, criterion='entropy', max_depth=5, min_samples=1): # Set default parameters\n",
        "        self.tree = None\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples = min_samples\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        classes, counts = np.unique(y, return_counts=True)\n",
        "        probs = counts / len(y)\n",
        "        entropy = -np.sum(probs * np.log2(probs))\n",
        "        return entropy\n",
        "\n",
        "    def _gini(self, y):\n",
        "        classes, counts = np.unique(y, return_counts=True)\n",
        "        probs = counts / len(y)\n",
        "        gini = 1 - np.sum(probs ** 2)\n",
        "        return gini\n",
        "\n",
        "    def _split_dataset(self, X, y, feature_idx, threshold):\n",
        "        left_mask = X[:, feature_idx] <= threshold\n",
        "        right_mask = ~left_mask\n",
        "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
        "\n",
        "    def _calculate_split_criteria(self, y, y_left, y_right):\n",
        "        if self.criterion == 'entropy':\n",
        "            information_gain = self._entropy(y) - (len(y_left) / len(y)) * self._entropy(y_left) - (len(y_right) / len(y)) * self._entropy(y_right)\n",
        "            return information_gain\n",
        "        elif self.criterion == 'gain_ratio':\n",
        "            information_gain = self._entropy(y) - (len(y_left) / len(y)) * self._entropy(y_left) - (len(y_right) / len(y)) * self._entropy(y_right)\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "                split_info = -((len(y_left) / len(y)) * np.log2(len(y_left) / len(y)) + (len(y_right) / len(y)) * np.log2(len(y_right) / len(y)))\n",
        "            return information_gain / split_info if split_info != 0 else 0  # Avoid division by zero\n",
        "        elif self.criterion == 'gini':\n",
        "            gini_impurity = self._gini(y) - (len(y_left) / len(y)) * self._gini(y_left) - (len(y_right) / len(y)) * self._gini(y_right)\n",
        "            return gini_impurity\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        best_criteria = -1\n",
        "        best_feature_idx = None\n",
        "        best_threshold = None\n",
        "        for feature_idx in range(X.shape[1]):\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            for threshold in thresholds:\n",
        "                X_left, X_right, y_left, y_right = self._split_dataset(X, y, feature_idx, threshold)\n",
        "                criteria = self._calculate_split_criteria(y, y_left, y_right)\n",
        "                if criteria > best_criteria:\n",
        "                    best_criteria = criteria\n",
        "                    best_feature_idx = feature_idx\n",
        "                    best_threshold = threshold\n",
        "        return best_feature_idx, best_threshold\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        if (self.max_depth is not None and depth >= self.max_depth) or len(np.unique(y)) == 1 or len(y) < self.min_samples:\n",
        "            return np.bincount(y).argmax(), None, None, None\n",
        "        feature_idx, threshold = self._find_best_split(X, y)\n",
        "        if feature_idx is None or threshold is None:\n",
        "            return np.bincount(y).argmax(), None, None, None\n",
        "        X_left, X_right, y_left, y_right = self._split_dataset(X, y, feature_idx, threshold)\n",
        "        left_subtree = self._build_tree(X_left, y_left, depth + 1)\n",
        "        right_subtree = self._build_tree(X_right, y_right, depth + 1)\n",
        "        return (feature_idx, threshold, left_subtree, right_subtree)\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _predict_instance(self, x, tree):\n",
        "        if tree[1] is None:  # Leaf node\n",
        "            return tree[0]\n",
        "        feature_idx, threshold, left_subtree, right_subtree = tree\n",
        "        if threshold is not None:  # Ensure threshold is not None\n",
        "            if x[feature_idx] <= threshold:\n",
        "                return self._predict_instance(x, left_subtree)\n",
        "            else:\n",
        "                return self._predict_instance(x, right_subtree)\n",
        "        else:  # Handle case where threshold is None\n",
        "            # If threshold is None, we cannot make a decision, so return a default value\n",
        "            # You can decide what value to return here based on your application\n",
        "            return -1  # For example, returning -1 indicates an unknown classification\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            prediction = self._predict_instance(x, self.tree)\n",
        "            predictions.append(prediction)\n",
        "        return np.array(predictions)\n",
        "\n",
        "# To calculate the accuracy score\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIJwsNr9qPF-"
      },
      "source": [
        "## Random Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "9Amnteo39XLo"
      },
      "outputs": [],
      "source": [
        "# Function to split data into train and test sets\n",
        "def train_test_split(X, y, test_size=0.3): # Use 30% for testing\n",
        "    num_samples = X.shape[0]\n",
        "    num_test_samples = int(test_size * num_samples)\n",
        "\n",
        "    test_indices = np.random.choice(num_samples, num_test_samples, replace=False)\n",
        "    train_indices = np.setdiff1d(np.arange(num_samples), test_indices)\n",
        "\n",
        "    X_train, X_test = X[train_indices], X[test_indices]\n",
        "    y_train, y_test = y[train_indices], y[test_indices]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X = df[names].values # significant_features\n",
        "y = df['Target'].values#.reshape(-1,1)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DID2Mv2-qS2S"
      },
      "source": [
        "## View Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "TebuE5tQo8n5",
        "outputId": "899fce6f-478a-4f73-c564-cebeae673cc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy using Entropy: 0.7490580256217031\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfor criterion in criteria:\\n    tree = DecisionTree(criterion)\\n    tree.fit(X_train, y_train)\\n    y_pred = tree.predict(X_test)\\n    acc = accuracy(y_test, y_pred)\\n    print(f\"Accuracy using {criterion.capitalize()}: {acc}\")\\nprint()\\n'"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tree_entropy = DecisionTree(criterion='entropy', max_depth=5, min_samples=2)\n",
        "tree_entropy.fit(X_train, y_train)\n",
        "y_pred = tree_entropy.predict(X_test)\n",
        "acc = accuracy(y_test, y_pred)\n",
        "print(f\"Accuracy using Entropy: {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpCSdt-ctG3g",
        "outputId": "e01e8721-b0bd-4586-9693-a17dbd762414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy using Gain Ratio: 0.7445365486058779\n"
          ]
        }
      ],
      "source": [
        "tree_gainRatio = DecisionTree(criterion='gain_ratio', max_depth=7, min_samples=4)\n",
        "tree_gainRatio.fit(X_train, y_train)\n",
        "y_pred = tree_gainRatio.predict(X_test)\n",
        "acc = accuracy(y_test, y_pred)\n",
        "print(f\"Accuracy using Gain Ratio: {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrwWBxGYtGwb",
        "outputId": "f6b5c69b-015f-44f4-f648-5234152bd112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy using Gini Impurity: 0.7430293896006028\n"
          ]
        }
      ],
      "source": [
        "tree_gini = DecisionTree(criterion='gini', max_depth=3, min_samples=2)\n",
        "tree_gini.fit(X_train, y_train)\n",
        "y_pred = tree_gini.predict(X_test)\n",
        "acc = accuracy(y_test, y_pred)\n",
        "print(f\"Accuracy using Gini Impurity: {acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Howk3sjNxchR"
      },
      "source": [
        "### Summary:\n",
        "- Accuracy using Entropy: 74.91%\n",
        "- Accuracy using Gain Ratio: 74.45%\n",
        "- Accuracy using Gini Impurity: 74.30%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmlCpuC9wrbs"
      },
      "source": [
        "### Why I used pre-pruning technique?\n",
        "Using pre-pruning techniques, even in scenarios where overfitting may not be a significant concern, can still provide tangible benefits in terms of simplicity, efficiency, robustness, and future-proofing of your machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwi_T-QEp43h"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "- The three criteria, Entropy, Gain Ratio, and Gini Impurity, showed very similar accuracies.\n",
        "- The choice of hyperparameters also varied across the criteria, with different optimal values for max_depth and min_samples_split.\n",
        "\n",
        "\n",
        "Further analysis and experimentation may be required to understand the performance differences across the criteria and fine-tune the model for better accuracy.\n",
        "\n",
        "(If post-pruning had been available, better results could have been expected. However, since only pre-pruning technique was mentioned in the assignment documentation, it was not performed.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
